{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discord Question Generation\n",
    "\n",
    "Original code: https://github.com/salesforce/discord_questions/blob/d7cbd514895bdfbb54782645909eea70fe1435b3/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Version\n",
    "**Incomplete**\n",
    "\n",
    "Using their pipeline, which uses their pre-trained models to generate the discord questions, generate answers, and consolidate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the entire pipeline they used to create discord questions has more to it. Like removing duplicates, checking if the discord question actually create questions that made different sources answer differently (or, the sources can answer them in the first place), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa15b800b48e4721ad3b346f822b4698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e843095dbda44a8084c8842b99dffa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How will the global economy be affected by the crises?']\n",
      "['Why is the world in recession?']\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/Salesforce/discord_qg\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "qg_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/discord_qg\")\n",
    "qg_model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/discord_qg\")\n",
    "\n",
    "paragraph = \"The International Monetary Fund warned on Tuesday that colliding pressures from inflation, war-driven energy and food crises and sharply higher interest rates were pushing the world to the brink of recession and threatening financial market stability.\"\n",
    "\n",
    "for start_word in [\"How\", \"Why\"]:\n",
    "    encoder_ids = qg_tokenizer.batch_encode_plus([paragraph], add_special_tokens=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    decoder_input_ids = qg_tokenizer.batch_encode_plus([start_word], add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"][:, :-1]\n",
    "    model_output = qg_model.generate(**encoder_ids, decoder_input_ids=decoder_input_ids, max_length=20)\n",
    "    generated_questions = qg_tokenizer.batch_decode(model_output, skip_special_tokens=True)\n",
    "\n",
    "    print(generated_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/Salesforce/discord_qa\n",
    "# Load model directly\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/discord_qa\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"Salesforce/discord_qa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidation process explanation\n",
    "(note: the entire consolidation is in their github repo. below is just on using the language model they trained to calculate similarity)\n",
    "\n",
    "they pair up all the possible pair-wise combinations of paragraphs. For every two paragraph (`p1` and `p2`) with answer fields (i assume here it's `a1`, `a2`), they create the text\n",
    "`text = \"<question> <sep> <p1['answer']> <sep> <p2['answer']>\"`\n",
    "which are then tokenized, padded, and passed through the model to obtain logits. The scores extracted from the logits are used to rank or measure the similarity of the two answers in the pair.\n",
    "\n",
    "Once all pair-wise paragraph combination is calculated, they create a weight matrix, then use a threshold to determine if an edge should exist between two paragraphs, then use the weight matrix to create a graph. Finally run *Louvain algorithm* on the graph to determine clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import itertools\n",
    "\n",
    "ae_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/qa_consolidation\")\n",
    "ae_model = AutoModelForSequenceClassification.from_pretrained(\"Salesforce/qa_consolidation\").eval()\n",
    "\n",
    "question = \"When will the recession happen?\"\n",
    "answers = [\"probably next January\", \"never\", \"we're already in a recession\", \"it won't happen\", \"it's going on right now\", \"not before next year\", \"upcoming January-March\"]\n",
    "dataset = [{\"a1\": a1, \"a2\": a2, \"input\": \"%s <sep> %s <sep> %s\" % (question, a1, a2)} for a1, a2 in itertools.combinations(answers, 2)]\n",
    "\n",
    "input_ids = ae_tokenizer.batch_encode_plus([d[\"input\"] for d in dataset], add_special_tokens=False, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "scores = ae_model(input_ids=input_ids)[\"logits\"][:, 0].tolist()\n",
    "for d, score in zip(dataset, scores):\n",
    "    d[\"score\"] = score\n",
    "\n",
    "for d in sorted(dataset, key=lambda d: -d[\"score\"]):\n",
    "    print(\"[Score: %.3f] %s\" % (d[\"score\"], d[\"input\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot/Few-Shot Version\n",
    "\n",
    "gonna try to recreate the Discord Question creation process just using base models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
